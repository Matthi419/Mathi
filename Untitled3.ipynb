{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z93R_xe87RC5",
        "outputId": "e64f15d1-7794-4025-f077-64fc7eba70da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - loss: 9968.5986 - val_loss: 4061.6184\n",
            "Epoch 2/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 3964.0850 - val_loss: 3741.4451\n",
            "Epoch 3/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 3770.2537 - val_loss: 3518.1062\n",
            "Epoch 4/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - loss: 3533.0977 - val_loss: 3315.0466\n",
            "Epoch 5/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 3495.5117 - val_loss: 3229.4778\n",
            "Epoch 6/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 3199.5391 - val_loss: 3068.3818\n",
            "Epoch 7/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 3069.8728 - val_loss: 2889.0525\n",
            "Epoch 8/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 2758.0681 - val_loss: 2264.3894\n",
            "Epoch 9/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 2051.9277 - val_loss: 1778.0859\n",
            "Epoch 10/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 1674.4021 - val_loss: 1646.2277\n",
            "Epoch 11/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 1540.6417 - val_loss: 1533.8016\n",
            "Epoch 12/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 1492.8358 - val_loss: 1510.0554\n",
            "Epoch 13/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 1448.7429 - val_loss: 1458.7144\n",
            "Epoch 14/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 1415.6746 - val_loss: 1436.0187\n",
            "Epoch 15/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 1430.4194 - val_loss: 1481.4581\n",
            "Epoch 16/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 1361.7632 - val_loss: 1450.3424\n",
            "Epoch 17/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 1359.4136 - val_loss: 1376.1021\n",
            "Epoch 18/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 1331.8290 - val_loss: 1369.4189\n",
            "Epoch 19/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 1325.3761 - val_loss: 1406.0801\n",
            "Epoch 20/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 1318.3555 - val_loss: 1333.1268\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step\n",
            "\u001b[1m798/798\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "XGBoost (mit Embeddings) Mean Squared Error: 1327.28\n",
            "XGBoost (mit Embeddings) R^2 Score: 0.93\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense\n",
        "import xgboost as xgb\n",
        "\n",
        "# 1. Daten einlesen und vorbereiten\n",
        "bakery_data = pd.read_csv('/content/bakery_data (3).csv')\n",
        "bakery_target = pd.read_csv('/content/bakery_target (3).csv')\n",
        "\n",
        "# Zielvariable hinzufügen\n",
        "bakery_data['demand'] = bakery_target['demand']\n",
        "\n",
        "# Entfernen der 'date' Spalte\n",
        "bakery_data = bakery_data.drop('date', axis=1)\n",
        "\n",
        "# Kategorische Variablen\n",
        "categorical_columns = ['weekday', 'month', 'store', 'product']\n",
        "\n",
        "# Label Encoding der kategorischen Variablen\n",
        "label_encoders = {}\n",
        "for column in categorical_columns:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    bakery_data[column] = label_encoders[column].fit_transform(bakery_data[column])\n",
        "\n",
        "# Numerische und Zielvariablen definieren\n",
        "X_numeric = bakery_data.drop(categorical_columns + ['demand'], axis=1)\n",
        "y = bakery_data['demand']\n",
        "\n",
        "# Normalisierung der numerischen Features\n",
        "scaler = StandardScaler()\n",
        "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testsets (für numerische und kategorische Variablen)\n",
        "X_train_num, X_test_num, X_train_cat, X_test_cat, y_train, y_test = train_test_split(\n",
        "    X_numeric_scaled, bakery_data[categorical_columns], y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 2. Embeddings-Generierung mit einem Neuronalen Netz\n",
        "# Embedding Input-Größen\n",
        "embedding_sizes = {'weekday': 7, 'month': 12, 'store': 10, 'product': 50}  # Beispielhafte Input-Dimensionen\n",
        "\n",
        "input_layers = []\n",
        "embedding_layers = []\n",
        "\n",
        "# Für jede kategorische Variable eine Embedding-Schicht definieren\n",
        "for col in categorical_columns:\n",
        "    input_layer = Input(shape=(1,))\n",
        "    input_layers.append(input_layer)\n",
        "    vocab_size = bakery_data[col].nunique() + 1  # Anzahl der eindeutigen Werte + 1\n",
        "    embed_size = embedding_sizes[col]\n",
        "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_size)(input_layer)\n",
        "    embedding_layer = Flatten()(embedding_layer)\n",
        "    embedding_layers.append(embedding_layer)\n",
        "\n",
        "# Numerische Eingaben\n",
        "input_numeric = Input(shape=(X_train_num.shape[1],))\n",
        "input_layers.append(input_numeric)\n",
        "\n",
        "# Kombination von Embeddings und numerischen Features\n",
        "all_features = Concatenate()(embedding_layers + [input_numeric])\n",
        "\n",
        "# Dichtes neuronales Netz zur Feature-Extraktion\n",
        "x = Dense(64, activation='relu')(all_features)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "output = Dense(1, activation='linear')(x)\n",
        "\n",
        "# Modell zur Extraktion der Embeddings\n",
        "embedding_model = Model(inputs=input_layers, outputs=output)\n",
        "embedding_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Training des Embedding-Modells\n",
        "X_train_inputs = [X_train_cat[col].values for col in categorical_columns] + [X_train_num]\n",
        "X_test_inputs = [X_test_cat[col].values for col in categorical_columns] + [X_test_num]\n",
        "embedding_model.fit(X_train_inputs, y_train, epochs=20, batch_size=32, validation_data=(X_test_inputs, y_test))\n",
        "\n",
        "# 3. Embeddings extrahieren\n",
        "feature_extractor = Model(inputs=embedding_model.inputs, outputs=embedding_model.layers[-2].output)\n",
        "X_train_embeddings = feature_extractor.predict(X_train_inputs)\n",
        "X_test_embeddings = feature_extractor.predict(X_test_inputs)\n",
        "\n",
        "# 4. XGBoost-Modell trainieren (mit Embeddings als Input)\n",
        "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,\n",
        "                          max_depth=5, alpha=10, n_estimators=100, random_state=42)\n",
        "xg_reg.fit(X_train_embeddings, y_train)\n",
        "\n",
        "# Vorhersagen mit XGBoost-Modell\n",
        "y_pred_xgb = xg_reg.predict(X_test_embeddings)\n",
        "\n",
        "# Metriken für XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost (mit Embeddings) Mean Squared Error: {mse_xgb:.2f}\")\n",
        "print(f\"XGBoost (mit Embeddings) R^2 Score: {r2_xgb:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# 1. Daten einlesen und vorbereiten\n",
        "bakery_data = pd.read_csv('/content/bakery_data (3).csv')\n",
        "bakery_target = pd.read_csv('/content/bakery_target (3).csv')\n",
        "\n",
        "# Zielvariable hinzufügen\n",
        "bakery_data['demand'] = bakery_target['demand']\n",
        "\n",
        "# Entfernen der 'date' Spalte\n",
        "bakery_data = bakery_data.drop('date', axis=1)\n",
        "\n",
        "# Kategorische Variablen\n",
        "categorical_columns = ['weekday', 'month', 'store', 'product']\n",
        "\n",
        "# Label Encoding der kategorischen Variablen\n",
        "label_encoders = {}\n",
        "for column in categorical_columns:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    bakery_data[column] = label_encoders[column].fit_transform(bakery_data[column])\n",
        "\n",
        "# Numerische und Zielvariablen definieren\n",
        "X = bakery_data.drop(['demand'], axis=1)\n",
        "y = bakery_data['demand']\n",
        "\n",
        "# Normalisierung der Features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testsets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. XGBoost-Modell trainieren\n",
        "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,\n",
        "                          max_depth=5, alpha=10, n_estimators=100, random_state=42)\n",
        "xg_reg.fit(X_train, y_train)\n",
        "\n",
        "# Vorhersagen mit XGBoost-Modell\n",
        "y_pred_xgb = xg_reg.predict(X_test)\n",
        "\n",
        "# Metriken für XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb:.2f}\")\n",
        "print(f\"XGBoost R^2 Score: {r2_xgb:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWNrQWfe-h-b",
        "outputId": "9c244e7d-b800-431c-cef6-4be37f11f278"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Mean Squared Error: 6853.27\n",
            "XGBoost R^2 Score: 0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neuer Versuch"
      ],
      "metadata": {
        "id": "l-iJbWadUK1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense\n",
        "import xgboost as xgb\n",
        "\n",
        "# 1. Daten einlesen und vorbereiten\n",
        "bakery_data = pd.read_csv('/content/bakery_data (3).csv')\n",
        "bakery_target = pd.read_csv('/content/bakery_target (3).csv')\n",
        "\n",
        "# Zielvariable hinzufügen\n",
        "bakery_data['demand'] = bakery_target['demand']\n",
        "\n",
        "# Entfernen der 'date' Spalte\n",
        "bakery_data = bakery_data.drop('date', axis=1)\n",
        "\n",
        "# Kategorische Variablen\n",
        "categorical_columns = ['weekday', 'month', 'store', 'product']\n",
        "\n",
        "# Label Encoding der kategorischen Variablen\n",
        "label_encoders = {}\n",
        "for column in categorical_columns:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    bakery_data[column] = label_encoders[column].fit_transform(bakery_data[column])\n",
        "\n",
        "# Numerische und Zielvariablen definieren\n",
        "X_numeric = bakery_data.drop(categorical_columns + ['demand'], axis=1)\n",
        "y = bakery_data['demand']\n",
        "\n",
        "# Normalisierung der numerischen Features\n",
        "scaler = StandardScaler()\n",
        "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testsets (für numerische und kategorische Variablen)\n",
        "X_train_num, X_test_num, X_train_cat, X_test_cat, y_train, y_test = train_test_split(\n",
        "    X_numeric_scaled, bakery_data[categorical_columns], y, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "q-FCyvSOBM00"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Embeddings-Generierung mit einem Neuronalen Netz\n",
        "embedding_sizes = {'weekday': 7, 'month': 12, 'store': 10, 'product': 50}  # Beispielhafte Input-Dimensionen\n",
        "\n",
        "input_layers = []\n",
        "embedding_layers = []\n",
        "\n",
        "# Für jede kategorische Variable eine Embedding-Schicht definieren\n",
        "for col in categorical_columns:\n",
        "    input_layer = Input(shape=(1,))\n",
        "    input_layers.append(input_layer)\n",
        "    vocab_size = bakery_data[col].nunique() + 1  # Anzahl der eindeutigen Werte + 1\n",
        "    embed_size = embedding_sizes[col]\n",
        "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_size)(input_layer)\n",
        "    embedding_layer = Flatten()(embedding_layer)\n",
        "    embedding_layers.append(embedding_layer)\n",
        "\n",
        "# Numerische Eingaben\n",
        "input_numeric = Input(shape=(X_train_num.shape[1],))\n",
        "input_layers.append(input_numeric)\n",
        "\n",
        "# Kombination von Embeddings und numerischen Features\n",
        "all_features = Concatenate()(embedding_layers + [input_numeric])\n",
        "\n",
        "# Dichtes neuronales Netz zur Feature-Extraktion\n",
        "x = Dense(64, activation='relu')(all_features)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "output = Dense(1, activation='linear')(x)\n",
        "\n",
        "# Modell zur Extraktion der Embeddings\n",
        "embedding_model = Model(inputs=input_layers, outputs=output)\n",
        "embedding_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Training des Embedding-Modells\n",
        "X_train_inputs = [X_train_cat[col].values for col in categorical_columns] + [X_train_num]\n",
        "X_test_inputs = [X_test_cat[col].values for col in categorical_columns] + [X_test_num]\n",
        "embedding_model.fit(X_train_inputs, y_train, epochs=20, batch_size=32, validation_data=(X_test_inputs, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaQu2b9xCuKp",
        "outputId": "67c4f9fd-c2dd-484c-b7bb-d5236ffc1154"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - loss: 9905.9180 - val_loss: 4017.6182\n",
            "Epoch 2/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 3985.2932 - val_loss: 3673.2241\n",
            "Epoch 3/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6ms/step - loss: 3677.1362 - val_loss: 3500.9512\n",
            "Epoch 4/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 4ms/step - loss: 3553.4971 - val_loss: 3304.6511\n",
            "Epoch 5/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 3462.8237 - val_loss: 3137.6987\n",
            "Epoch 6/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 3259.5811 - val_loss: 3003.6157\n",
            "Epoch 7/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 2990.0562 - val_loss: 2619.5757\n",
            "Epoch 8/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 2493.4475 - val_loss: 2167.4153\n",
            "Epoch 9/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 2046.1041 - val_loss: 1897.1216\n",
            "Epoch 10/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 1820.5973 - val_loss: 1708.7985\n",
            "Epoch 11/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 1615.6508 - val_loss: 1549.3345\n",
            "Epoch 12/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 1548.4940 - val_loss: 1508.2129\n",
            "Epoch 13/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 1469.6942 - val_loss: 1473.6144\n",
            "Epoch 14/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 1412.0007 - val_loss: 1408.9897\n",
            "Epoch 15/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 1398.4515 - val_loss: 1386.7218\n",
            "Epoch 16/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - loss: 1368.3448 - val_loss: 1416.7529\n",
            "Epoch 17/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 1337.2710 - val_loss: 1391.1207\n",
            "Epoch 18/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 1307.0242 - val_loss: 1337.0374\n",
            "Epoch 19/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 1326.5751 - val_loss: 1334.6608\n",
            "Epoch 20/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 1283.0194 - val_loss: 1377.4260\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b886d7ad480>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Embeddings extrahieren\n",
        "feature_extractor = Model(inputs=embedding_model.inputs, outputs=embedding_model.layers[-2].output)\n",
        "X_train_embeddings = feature_extractor.predict(X_train_inputs)\n",
        "X_test_embeddings = feature_extractor.predict(X_test_inputs)\n",
        "\n",
        "# 4. XGBoost-Modell trainieren (mit Embeddings als Input)\n",
        "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,\n",
        "                          max_depth=5, alpha=10, n_estimators=100, random_state=42)\n",
        "xg_reg.fit(X_train_embeddings, y_train)\n",
        "\n",
        "# Vorhersagen mit XGBoost-Modell\n",
        "y_pred_xgb = xg_reg.predict(X_test_embeddings)\n",
        "\n",
        "# Metriken für XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost (mit Embeddings) Mean Squared Error: {mse_xgb:.2f}\")\n",
        "print(f\"XGBoost (mit Embeddings) R^2 Score: {r2_xgb:.2f}\")\n",
        "\n",
        "# 5. Vergleich mit Standalone XGBoost-Modell\n",
        "X_train_all = X_train_num\n",
        "X_test_all = X_test_num\n",
        "\n",
        "xg_reg_all = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,\n",
        "                              max_depth=5, alpha=10, n_estimators=100, random_state=42)\n",
        "xg_reg_all.fit(X_train_all, y_train)\n",
        "\n",
        "y_pred_xgb_all = xg_reg_all.predict(X_test_all)\n",
        "\n",
        "mse_xgb_all = mean_squared_error(y_test, y_pred_xgb_all)\n",
        "r2_xgb_all = r2_score(y_test, y_pred_xgb_all)\n",
        "\n",
        "print(f\"XGBoost (nur numerische Features) Mean Squared Error: {mse_xgb_all:.2f}\")\n",
        "print(f\"XGBoost (nur numerische Features) R^2 Score: {r2_xgb_all:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBc3BUuaDI44",
        "outputId": "9b2ecc26-d1b8-42bb-c391-13390ca791b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step\n",
            "\u001b[1m798/798\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
            "XGBoost (mit Embeddings) Mean Squared Error: 1310.45\n",
            "XGBoost (mit Embeddings) R^2 Score: 0.93\n",
            "XGBoost (nur numerische Features) Mean Squared Error: 18628.96\n",
            "XGBoost (nur numerische Features) R^2 Score: 0.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Daten einlesen und vorbereiten\n",
        "bakery_data = pd.read_csv('/content/bakery_data (3).csv')\n",
        "bakery_target = pd.read_csv('/content/bakery_target (3).csv')\n",
        "\n",
        "# Zielvariable hinzufügen\n",
        "bakery_data['demand'] = bakery_target['demand']\n",
        "\n",
        "# Entfernen der 'date' Spalte\n",
        "bakery_data = bakery_data.drop('date', axis=1)\n",
        "\n",
        "# Kategorische Variablen\n",
        "categorical_columns = ['weekday', 'month', 'store', 'product']\n",
        "\n",
        "# Label Encoding der kategorischen Variablen\n",
        "label_encoders = {}\n",
        "for column in categorical_columns:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    bakery_data[column] = label_encoders[column].fit_transform(bakery_data[column])\n",
        "\n",
        "# Alle Features und Zielvariable\n",
        "X = bakery_data.drop(['demand'], axis=1)\n",
        "y = bakery_data['demand']\n",
        "\n",
        "# Normalisierung der Features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testsets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "5WKY8c-6UiiW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. XGBoost-Modell trainieren\n",
        "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,\n",
        "                          max_depth=5, alpha=10, n_estimators=100, random_state=42)\n",
        "xg_reg.fit(X_train, y_train)\n",
        "\n",
        "# Vorhersagen mit XGBoost-Modell\n",
        "y_pred_xgb = xg_reg.predict(X_test)\n",
        "\n",
        "# Metriken für XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb:.2f}\")\n",
        "print(f\"XGBoost R^2 Score: {r2_xgb:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vPAoY6hUkK3",
        "outputId": "70a53c80-bf51-4707-f8c6-84f539bf6038"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Mean Squared Error: 6853.27\n",
            "XGBoost R^2 Score: 0.64\n"
          ]
        }
      ]
    }
  ]
}