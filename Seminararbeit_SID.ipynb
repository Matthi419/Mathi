{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Datenverarbeitung"
      ],
      "metadata": {
        "id": "ynp8klEbw28H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "mWbgnHUutJD5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Funktion zum Einlesen und Vorbereiten der Daten für einen einzelnen Datensatz\n",
        "def prepare_data(data_path, target_path, target_col, categorical_columns):\n",
        "    # Daten einlesen\n",
        "    data = pd.read_csv(data_path)\n",
        "    target = pd.read_csv(target_path)\n",
        "\n",
        "    # Zielvariable hinzufügen\n",
        "    data[target_col] = target[target_col]\n",
        "\n",
        "    # Entfernen der 'date' Spalte, falls vorhanden\n",
        "    if 'date' in data.columns:\n",
        "        data = data.drop('date', axis=1)\n",
        "\n",
        "    # Label Encoding der kategorischen Variablen\n",
        "    label_encoders = {}\n",
        "    for column in categorical_columns:\n",
        "        if column in data.columns:\n",
        "            label_encoders[column] = LabelEncoder()\n",
        "            data[column] = label_encoders[column].fit_transform(data[column])\n",
        "\n",
        "    # Alle Features und Zielvariable\n",
        "    X = data.drop([target_col], axis=1)\n",
        "    y = data[target_col]\n",
        "\n",
        "    # Normalisierung der Features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Aufteilen der Daten in Trainings- und Testsets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UYrvhJg3wz6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SID"
      ],
      "metadata": {
        "id": "wu9CQv50wqJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sid_path = '/content/SID_data.csv'\n",
        "sid_target_path = '/content/SID_target.csv'\n",
        "sid_categorical_columns = []\n"
      ],
      "metadata": {
        "id": "Drdj8fwqvabd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Pfade für den SID-Datensatz\n",
        "sid_path = '/content/SID_data.csv'\n",
        "sid_target_path = '/content/SID_target.csv'\n",
        "\n",
        "# Daten einlesen\n",
        "sid_data = pd.read_csv(sid_path)\n",
        "sid_target = pd.read_csv(sid_target_path)\n",
        "\n",
        "# Überprüfen der Spalten in den Ziel-Daten\n",
        "print(\"SID Target Columns:\")\n",
        "print(sid_target.columns)\n",
        "\n",
        "print(\"\\nSID Target Data Types:\")\n",
        "print(sid_target.dtypes)\n",
        "\n",
        "# Zielvariable hinzufügen\n",
        "sid_data['sales'] = sid_target['sales']\n",
        "\n",
        "# Entfernen von nicht benötigten Spalten (Beispiel: 'date', falls vorhanden)\n",
        "if 'date' in sid_data.columns:\n",
        "    sid_data = sid_data.drop('date', axis=1)\n",
        "\n",
        "# Kategorische Variablen\n",
        "sid_categorical_columns = ['weekday', 'month']  # 'date' wurde entfernt\n",
        "\n",
        "# Label Encoding der kategorischen Variablen\n",
        "label_encoders = {}\n",
        "for column in sid_categorical_columns:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    sid_data[column] = label_encoders[column].fit_transform(sid_data[column])\n",
        "\n",
        "# Alle Features und Zielvariable\n",
        "X_sid = sid_data.drop(['sales'], axis=1)\n",
        "y_sid = sid_data['sales']\n",
        "\n",
        "# Normalisierung der Features\n",
        "scaler = StandardScaler()\n",
        "X_sid_scaled = scaler.fit_transform(X_sid)\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testsets\n",
        "X_sid_train, X_sid_test, y_sid_train, y_sid_test = train_test_split(X_sid_scaled, y_sid, test_size=0.2, random_state=42)\n",
        "\n",
        "# Zeige die vorbereiteten Daten\n",
        "print(\"\\nSID Data (Training Set):\")\n",
        "print(pd.DataFrame(X_sid_train, columns=X_sid.columns).head())\n",
        "\n",
        "print(\"\\nSID Data (Test Set):\")\n",
        "print(pd.DataFrame(X_sid_test, columns=X_sid.columns).head())\n",
        "\n",
        "print(\"\\nSID Target (Training Set):\")\n",
        "print(y_sid_train.head())\n",
        "\n",
        "print(\"\\nSID Target (Test Set):\")\n",
        "print(y_sid_test.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJV3n0DIvyl0",
        "outputId": "45747b04-343b-4bd4-c619-21b4d5689d5d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SID Target Columns:\n",
            "Index(['sales'], dtype='object')\n",
            "\n",
            "SID Target Data Types:\n",
            "sales    int64\n",
            "dtype: object\n",
            "\n",
            "SID Data (Training Set):\n",
            "    weekday     month      year     store      item\n",
            "0  0.000000 -1.307378 -1.414795 -0.174078  0.519719\n",
            "1 -2.085094 -0.437381  1.414020 -1.218544 -0.589015\n",
            "2  3.127642 -1.017379 -1.414795  1.218544  1.697749\n",
            "3  2.085094  0.722616 -1.414795 -1.218544 -0.935495\n",
            "4  0.000000 -0.437381 -0.707591  0.174078 -0.103944\n",
            "\n",
            "SID Data (Test Set):\n",
            "   weekday     month      year     store      item\n",
            "0      0.0 -0.437381 -1.414795  1.566699  0.103944\n",
            "1      0.0  1.592613 -0.000387  0.174078 -1.489862\n",
            "2      0.0 -1.307378  1.414020  1.566699 -1.628453\n",
            "3      0.0  0.142617 -0.707591 -0.522233 -1.212678\n",
            "4      0.0  1.302614  1.414020  0.522233  0.658311\n",
            "\n",
            "SID Target (Training Set):\n",
            "591855    41\n",
            "295447    34\n",
            "909684    39\n",
            "202807    73\n",
            "429503    28\n",
            "Name: sales, dtype: int64\n",
            "\n",
            "SID Target (Test Set):\n",
            "491212     21\n",
            "64903      15\n",
            "36378     100\n",
            "133834     93\n",
            "633538     33\n",
            "Name: sales, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost-Modell ohne Embeddings"
      ],
      "metadata": {
        "id": "gQrP__ymxrl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Trainieren eines XGBoost-Modells ohne Embeddings\n",
        "xgb_model_no_embeddings = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
        "xgb_model_no_embeddings.fit(X_sid_train, y_sid_train)\n",
        "\n",
        "# Vorhersagen und Bewertung\n",
        "y_sid_pred_no_embeddings = xgb_model_no_embeddings.predict(X_sid_test)\n",
        "print(\"\\nMSE (SID, ohne Embeddings):\", mean_squared_error(y_sid_test, y_sid_pred_no_embeddings))\n",
        "print(\"R2 Score (SID, ohne Embeddings):\", r2_score(y_sid_test, y_sid_pred_no_embeddings))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z51Q1qyBxnLA",
        "outputId": "e33587f2-307e-4b16-d975-122de46b36a8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MSE (SID, ohne Embeddings): 104.76146167347257\n",
            "R2 Score (SID, ohne Embeddings): 0.8735065046573893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost-Modell mit Embeddings"
      ],
      "metadata": {
        "id": "5pp7ZwjxxyI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "# Definition eines einfachen MLP-Modells für Embeddings\n",
        "input_dim = X_sid_train.shape[1]\n",
        "embedding_dim = 10\n",
        "\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "dense_layer = Dense(64, activation='relu')(input_layer)\n",
        "embedding_layer = Dense(embedding_dim, activation='relu')(dense_layer)\n",
        "output_layer = Dense(1, activation='linear')(embedding_layer)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Training des Modells\n",
        "model.fit(X_sid_train, y_sid_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Extrahieren der Embeddings\n",
        "embedding_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
        "X_sid_train_embeddings = embedding_model.predict(X_sid_train)\n",
        "X_sid_test_embeddings = embedding_model.predict(X_sid_test)\n",
        "\n",
        "# XGBoost Modell mit den Embeddings trainieren\n",
        "xgb_model_with_embeddings = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
        "xgb_model_with_embeddings.fit(X_sid_train_embeddings, y_sid_train)\n",
        "\n",
        "# Vorhersagen und Bewertung\n",
        "y_sid_pred_with_embeddings = xgb_model_with_embeddings.predict(X_sid_test_embeddings)\n",
        "print(\"\\nMSE (SID, mit Embeddings):\", mean_squared_error(y_sid_test, y_sid_pred_with_embeddings))\n",
        "print(\"R2 Score (SID, mit Embeddings):\", r2_score(y_sid_test, y_sid_pred_with_embeddings))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWJm9V0Bxt_n",
        "outputId": "fffed42e-85c0-4075-8e98-e1a52f39c1bc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m18260/18260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step - loss: 917.7158 - val_loss: 643.9894\n",
            "Epoch 2/10\n",
            "\u001b[1m18260/18260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 2ms/step - loss: 641.9636 - val_loss: 628.8307\n",
            "Epoch 3/10\n",
            "\u001b[1m18260/18260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 2ms/step - loss: 620.6719 - val_loss: 600.4889\n",
            "Epoch 4/10\n",
            "\u001b[1m18260/18260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3ms/step - loss: 595.4899 - val_loss: 586.2525\n",
            "Epoch 5/10\n",
            "\u001b[1m18260/18260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2ms/step - loss: 574.7885 - val_loss: 565.3560\n",
            "Epoch 6/10\n",
            "\u001b[1m18260/18260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2ms/step - loss: 558.7922 - val_loss: 549.6039\n",
            "Epoch 7/10\n",
            "\u001b[1m18260/18260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3ms/step - loss: 548.2467 - val_loss: 548.1833\n",
            "Epoch 8/10\n",
            "\u001b[1m18260/18260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2ms/step - loss: 537.2753 - val_loss: 528.7866\n",
            "Epoch 9/10\n",
            "\u001b[1m18260/18260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2ms/step - loss: 524.7624 - val_loss: 524.1318\n",
            "Epoch 10/10\n",
            "\u001b[1m18260/18260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2ms/step - loss: 520.5774 - val_loss: 517.1297\n",
            "\u001b[1m22825/22825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1ms/step\n",
            "\u001b[1m5707/5707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step\n",
            "\n",
            "MSE (SID, mit Embeddings): 401.6025803684698\n",
            "R2 Score (SID, mit Embeddings): 0.5150877687469024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YAZ"
      ],
      "metadata": {
        "id": "14QWxRjzwntA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yaz_path = '/content/yaz_data (3).csv'\n",
        "yaz_target_path = '/content/yaz_target (3).csv'\n",
        "yaz_categorical_columns = []\n"
      ],
      "metadata": {
        "id": "D0daD1g8veLd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Pfade für den YAZ-Datensatz\n",
        "yaz_path = '/content/yaz_data (3).csv'\n",
        "yaz_target_path = '/content/yaz_target (3).csv'\n",
        "\n",
        "# Daten einlesen\n",
        "yaz_data = pd.read_csv(yaz_path)\n",
        "yaz_target = pd.read_csv(yaz_target_path)\n",
        "\n",
        "# Überprüfen der Spalten in den YAZ-Daten\n",
        "print(\"YAZ Data Columns:\")\n",
        "print(yaz_data.columns)\n",
        "\n",
        "print(\"\\nYAZ Data Types:\")\n",
        "print(yaz_data.dtypes)\n",
        "\n",
        "# Überprüfen der Spalten in den YAZ-Ziel-Daten\n",
        "print(\"\\nYAZ Target Columns:\")\n",
        "print(yaz_target.columns)\n",
        "\n",
        "print(\"\\nYAZ Target Data Types:\")\n",
        "print(yaz_target.dtypes)\n",
        "\n",
        "# Zielvariablen hinzufügen\n",
        "# Da es mehrere Zielvariablen gibt, speichern wir sie in einem DataFrame\n",
        "yaz_data = pd.concat([yaz_data, yaz_target], axis=1)\n",
        "\n",
        "# Entfernen von nicht benötigten Spalten (Beispiel: 'date', falls vorhanden)\n",
        "if 'date' in yaz_data.columns:\n",
        "    yaz_data = yaz_data.drop('date', axis=1)\n",
        "\n",
        "# Kategorische Variablen\n",
        "yaz_categorical_columns = ['weekday', 'month']  # Beispielhafte Annahme\n",
        "\n",
        "# Label Encoding der kategorischen Variablen\n",
        "label_encoders = {}\n",
        "for column in yaz_categorical_columns:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    yaz_data[column] = label_encoders[column].fit_transform(yaz_data[column])\n",
        "\n",
        "# Alle Features und Zielvariablen\n",
        "X_yaz = yaz_data.drop(yaz_target.columns, axis=1)\n",
        "y_yaz = yaz_data[yaz_target.columns]\n",
        "\n",
        "# Normalisierung der Features\n",
        "scaler = StandardScaler()\n",
        "X_yaz_scaled = scaler.fit_transform(X_yaz)\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testsets\n",
        "X_yaz_train, X_yaz_test, y_yaz_train, y_yaz_test = train_test_split(X_yaz_scaled, y_yaz, test_size=0.2, random_state=42)\n",
        "\n",
        "# Multi-Label-Klassifikator vorbereiten (Beispiel: RandomForestClassifier)\n",
        "model = MultiOutputClassifier(RandomForestClassifier())\n",
        "\n",
        "# Modell trainieren\n",
        "model.fit(X_yaz_train, y_yaz_train)\n",
        "\n",
        "# Vorhersagen machen\n",
        "y_yaz_pred = model.predict(X_yaz_test)\n",
        "\n",
        "# Metriken berechnen\n",
        "for i, column in enumerate(yaz_target.columns):\n",
        "    print(f\"Accuracy for {column}: {accuracy_score(y_yaz_test[column], y_yaz_pred[:, i])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MduVGXVVwY_e",
        "outputId": "66b2b2b6-ce97-4fa5-8166-2c3822b7a2a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YAZ Data Columns:\n",
            "Index(['date', 'weekday', 'month', 'year', 'is_holiday', 'is_closed',\n",
            "       'weekend', 'wind', 'clouds', 'rain', 'sunshine', 'temperature'],\n",
            "      dtype='object')\n",
            "\n",
            "YAZ Data Types:\n",
            "date            object\n",
            "weekday         object\n",
            "month           object\n",
            "year             int64\n",
            "is_holiday       int64\n",
            "is_closed        int64\n",
            "weekend          int64\n",
            "wind           float64\n",
            "clouds         float64\n",
            "rain           float64\n",
            "sunshine         int64\n",
            "temperature    float64\n",
            "dtype: object\n",
            "\n",
            "YAZ Target Columns:\n",
            "Index(['calamari', 'fish', 'shrimp', 'chicken', 'koefte', 'lamb', 'steak'], dtype='object')\n",
            "\n",
            "YAZ Target Data Types:\n",
            "calamari    int64\n",
            "fish        int64\n",
            "shrimp      int64\n",
            "chicken     int64\n",
            "koefte      int64\n",
            "lamb        int64\n",
            "steak       int64\n",
            "dtype: object\n",
            "Accuracy for calamari: 0.1437908496732026\n",
            "Accuracy for fish: 0.11764705882352941\n",
            "Accuracy for shrimp: 0.0784313725490196\n",
            "Accuracy for chicken: 0.026143790849673203\n",
            "Accuracy for koefte: 0.0784313725490196\n",
            "Accuracy for lamb: 0.06535947712418301\n",
            "Accuracy for steak: 0.0457516339869281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost-Modell ohne Embeddings"
      ],
      "metadata": {
        "id": "7CgR-ylLyXK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Pfade für den YAZ-Datensatz\n",
        "yaz_path = '/content/yaz_data (3).csv'\n",
        "yaz_target_path = '/content/yaz_target (3).csv'\n",
        "\n",
        "# Daten einlesen\n",
        "yaz_data = pd.read_csv(yaz_path)\n",
        "yaz_target = pd.read_csv(yaz_target_path)\n",
        "\n",
        "# Zielvariablen hinzufügen\n",
        "yaz_data = pd.concat([yaz_data, yaz_target], axis=1)\n",
        "\n",
        "# Entfernen von nicht benötigten Spalten (Beispiel: 'date', falls vorhanden)\n",
        "if 'date' in yaz_data.columns:\n",
        "    yaz_data = yaz_data.drop('date', axis=1)\n",
        "\n",
        "# Kategorische Variablen\n",
        "yaz_categorical_columns = ['weekday', 'month']  # Beispielhafte Annahme\n",
        "\n",
        "# Label Encoding der kategorischen Variablen\n",
        "label_encoders = {}\n",
        "for column in yaz_categorical_columns:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    yaz_data[column] = label_encoders[column].fit_transform(yaz_data[column])\n",
        "\n",
        "# Alle Features und Zielvariablen\n",
        "X_yaz = yaz_data.drop(yaz_target.columns, axis=1)\n",
        "y_yaz = yaz_data[yaz_target.columns]\n",
        "\n",
        "# Normalisierung der Features\n",
        "scaler = StandardScaler()\n",
        "X_yaz_scaled = scaler.fit_transform(X_yaz)\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testsets\n",
        "X_yaz_train, X_yaz_test, y_yaz_train, y_yaz_test = train_test_split(X_yaz_scaled, y_yaz, test_size=0.2, random_state=42)\n",
        "\n",
        "# Multi-Output-Regressionsmodell vorbereiten\n",
        "model_no_embeddings = MultiOutputRegressor(XGBRegressor(objective='reg:squarederror', n_estimators=100))\n",
        "\n",
        "# Modell trainieren\n",
        "model_no_embeddings.fit(X_yaz_train, y_yaz_train)\n",
        "\n",
        "# Vorhersagen machen\n",
        "y_yaz_pred_no_embeddings = model_no_embeddings.predict(X_yaz_test)\n",
        "\n",
        "# Metriken berechnen\n",
        "for i, column in enumerate(yaz_target.columns):\n",
        "    mse_no_embeddings = mean_squared_error(y_yaz_test[column], y_yaz_pred_no_embeddings[:, i])\n",
        "    print(f\"Mean Squared Error für {column} (ohne Embeddings): {mse_no_embeddings}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zCWTLuryP8A",
        "outputId": "4ab0dd2b-98e3-4072-8880-05646e53afd2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error für calamari (ohne Embeddings): 7.156469408848473\n",
            "Mean Squared Error für fish (ohne Embeddings): 9.19195603823591\n",
            "Mean Squared Error für shrimp (ohne Embeddings): 18.713473084944567\n",
            "Mean Squared Error für chicken (ohne Embeddings): 112.33111819097934\n",
            "Mean Squared Error für koefte (ohne Embeddings): 61.0968887534827\n",
            "Mean Squared Error für lamb (ohne Embeddings): 94.04143190533074\n",
            "Mean Squared Error für steak (ohne Embeddings): 67.84180353348987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost-Modell mit Embeddings"
      ],
      "metadata": {
        "id": "7K_lofCeyfYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Pfade für den YAZ-Datensatz\n",
        "yaz_path = '/content/yaz_data (3).csv'\n",
        "yaz_target_path = '/content/yaz_target (3).csv'\n",
        "\n",
        "# Daten einlesen\n",
        "yaz_data = pd.read_csv(yaz_path)\n",
        "yaz_target = pd.read_csv(yaz_target_path)\n",
        "\n",
        "# Zielvariablen hinzufügen\n",
        "yaz_data = pd.concat([yaz_data, yaz_target], axis=1)\n",
        "\n",
        "# Entfernen von nicht benötigten Spalten (Beispiel: 'date', falls vorhanden)\n",
        "if 'date' in yaz_data.columns:\n",
        "    yaz_data = yaz_data.drop('date', axis=1)\n",
        "\n",
        "# Kategorische Variablen\n",
        "yaz_categorical_columns = ['weekday', 'month']  # Beispielhafte Annahme\n",
        "\n",
        "# Label Encoding der kategorischen Variablen\n",
        "label_encoders = {}\n",
        "for column in yaz_categorical_columns:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    yaz_data[column] = label_encoders[column].fit_transform(yaz_data[column])\n",
        "\n",
        "# Alle Features und Zielvariablen\n",
        "X_yaz = yaz_data.drop(yaz_target.columns, axis=1)\n",
        "y_yaz = yaz_data[yaz_target.columns]\n",
        "\n",
        "# Normalisierung der Features\n",
        "scaler = StandardScaler()\n",
        "X_yaz_scaled = scaler.fit_transform(X_yaz)\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testsets\n",
        "X_yaz_train, X_yaz_test, y_yaz_train, y_yaz_test = train_test_split(X_yaz_scaled, y_yaz, test_size=0.2, random_state=42)\n",
        "\n",
        "# Einfaches MLP-Modell für Embeddings\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "input_dim = X_yaz_train.shape[1]\n",
        "embedding_dim = 10\n",
        "\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "dense_layer = Dense(64, activation='relu')(input_layer)\n",
        "embedding_layer = Dense(embedding_dim, activation='relu')(dense_layer)\n",
        "output_layer = Dense(1, activation='linear')(embedding_layer)\n",
        "\n",
        "mlp_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "mlp_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Training des MLP-Modells\n",
        "mlp_model.fit(X_yaz_train, y_yaz_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Extrahieren der Embeddings\n",
        "embedding_model = Model(inputs=mlp_model.input, outputs=mlp_model.layers[-2].output)\n",
        "X_yaz_train_embeddings = embedding_model.predict(X_yaz_train)\n",
        "X_yaz_test_embeddings = embedding_model.predict(X_yaz_test)\n",
        "\n",
        "# XGBoost Modell mit den Embeddings trainieren\n",
        "model_with_embeddings = MultiOutputRegressor(XGBRegressor(objective='reg:squarederror', n_estimators=100))\n",
        "model_with_embeddings.fit(X_yaz_train_embeddings, y_yaz_train)\n",
        "\n",
        "# Vorhersagen machen\n",
        "y_yaz_pred_with_embeddings = model_with_embeddings.predict(X_yaz_test_embeddings)\n",
        "\n",
        "# Metriken berechnen\n",
        "for i, column in enumerate(yaz_target.columns):\n",
        "    mse_with_embeddings = mean_squared_error(y_yaz_test[column], y_yaz_pred_with_embeddings[:, i])\n",
        "    print(f\"Mean Squared Error für {column} (mit Embeddings): {mse_with_embeddings}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dTqaFPNybkY",
        "outputId": "0844fd62-e7d4-48ac-ba48-4c640985ff44"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 481.9826 - val_loss: 568.6637\n",
            "Epoch 2/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 481.3294 - val_loss: 545.0606\n",
            "Epoch 3/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 445.7200 - val_loss: 518.6108\n",
            "Epoch 4/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 427.5568 - val_loss: 488.0193\n",
            "Epoch 5/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 400.6920 - val_loss: 453.6962\n",
            "Epoch 6/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 365.0493 - val_loss: 415.8508\n",
            "Epoch 7/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 317.8142 - val_loss: 375.3152\n",
            "Epoch 8/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 291.4059 - val_loss: 335.4407\n",
            "Epoch 9/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 268.5439 - val_loss: 299.3655\n",
            "Epoch 10/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 246.8666 - val_loss: 270.8137\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Mean Squared Error für calamari (mit Embeddings): 9.206548768903726\n",
            "Mean Squared Error für fish (mit Embeddings): 9.816486450326655\n",
            "Mean Squared Error für shrimp (mit Embeddings): 23.226553875413813\n",
            "Mean Squared Error für chicken (mit Embeddings): 188.27895548650906\n",
            "Mean Squared Error für koefte (mit Embeddings): 90.29447571921618\n",
            "Mean Squared Error für lamb (mit Embeddings): 184.19696544516916\n",
            "Mean Squared Error für steak (mit Embeddings): 98.36758009450608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP ohne Embeddings"
      ],
      "metadata": {
        "id": "avw__nL0yjav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Daten einlesen\n",
        "sid_data = pd.read_csv('/content/SID_data.csv')\n",
        "sid_target = pd.read_csv('/content/SID_target.csv')\n",
        "\n",
        "# Überprüfen der Spalten in den Ziel-Daten\n",
        "print(\"SID Target Columns:\")\n",
        "print(sid_target.columns)\n",
        "\n",
        "print(\"\\nSID Target Data Types:\")\n",
        "print(sid_target.dtypes)\n",
        "\n",
        "# Zielvariable hinzufügen\n",
        "sid_data['sales'] = sid_target['sales']\n",
        "\n",
        "# Entfernen von nicht benötigten Spalten (Beispiel: 'date', falls vorhanden)\n",
        "if 'date' in sid_data.columns:\n",
        "    sid_data = sid_data.drop('date', axis=1)\n",
        "\n",
        "# Kategorische Variablen (Beispielhaft 'weekday', 'month')\n",
        "sid_categorical_columns = ['weekday', 'month']\n",
        "\n",
        "# Label Encoding der kategorischen Variablen\n",
        "label_encoders = {}\n",
        "for column in sid_categorical_columns:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    sid_data[column] = label_encoders[column].fit_transform(sid_data[column])\n",
        "\n",
        "# Features (X) und Zielvariable (y)\n",
        "X_sid = sid_data.drop(['sales'], axis=1)\n",
        "y_sid = sid_data['sales']\n",
        "\n",
        "# Normalisierung der Features\n",
        "scaler = StandardScaler()\n",
        "X_sid_scaled = scaler.fit_transform(X_sid)\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testsets\n",
        "X_sid_train, X_sid_test, y_sid_train, y_sid_test = train_test_split(X_sid_scaled, y_sid, test_size=0.2, random_state=42)\n",
        "\n",
        "# Definieren des MLP-Modells ohne Embeddings\n",
        "class MLPWithoutEmbeddings(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(MLPWithoutEmbeddings, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialisiere das MLP-Modell\n",
        "input_size = X_sid_train.shape[1]\n",
        "mlp_model = MLPWithoutEmbeddings(input_size)\n",
        "\n",
        "# Optimizer und Loss-Funktion\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
        "\n",
        "# Konvertiere die Trainingsdaten in Torch-Tensoren\n",
        "X_sid_train_tensor = torch.tensor(X_sid_train, dtype=torch.float32)\n",
        "y_sid_train_tensor = torch.tensor(y_sid_train.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Trainingsloop\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    mlp_model.train()\n",
        "\n",
        "    # Vorhersagen und Verlust berechnen\n",
        "    optimizer.zero_grad()\n",
        "    outputs = mlp_model(X_sid_train_tensor)\n",
        "    loss = criterion(outputs, y_sid_train_tensor)\n",
        "\n",
        "    # Rückwärtspropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluierung auf Testset\n",
        "mlp_model.eval()\n",
        "X_sid_test_tensor = torch.tensor(X_sid_test, dtype=torch.float32)\n",
        "y_sid_test_tensor = torch.tensor(y_sid_test.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Vorhersagen\n",
        "with torch.no_grad():\n",
        "    y_pred = mlp_model(X_sid_test_tensor).numpy()\n",
        "\n",
        "# Berechnung des Mean Squared Error (MSE) und R²-Wert\n",
        "mse = mean_squared_error(y_sid_test, y_pred)\n",
        "r2 = r2_score(y_sid_test, y_pred)\n",
        "\n",
        "print(f'Mean Squared Error (MSE) auf Testset: {mse:.4f}')\n",
        "print(f'R²-Wert auf Testset: {r2:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dBMdM1jS_By",
        "outputId": "3dbe1117-ab48-44e6-e532-5f071f0905de"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SID Target Columns:\n",
            "Index(['sales'], dtype='object')\n",
            "\n",
            "SID Target Data Types:\n",
            "sales    int64\n",
            "dtype: object\n",
            "Epoch [10/100], Loss: 3500.9707\n",
            "Epoch [20/100], Loss: 3399.0120\n",
            "Epoch [30/100], Loss: 3256.3379\n",
            "Epoch [40/100], Loss: 3062.7607\n",
            "Epoch [50/100], Loss: 2811.4595\n",
            "Epoch [60/100], Loss: 2503.3369\n",
            "Epoch [70/100], Loss: 2151.3074\n",
            "Epoch [80/100], Loss: 1783.7434\n",
            "Epoch [90/100], Loss: 1443.7964\n",
            "Epoch [100/100], Loss: 1179.0358\n",
            "Mean Squared Error (MSE) auf Testset: 1158.5664\n",
            "R²-Wert auf Testset: -0.3989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP ohne Ebeddings Bakery"
      ],
      "metadata": {
        "id": "UiknqYMiU6fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# 1. Daten einlesen und vorbereiten\n",
        "bakery_data = pd.read_csv('/content/bakery_data (3).csv')\n",
        "bakery_target = pd.read_csv('/content/bakery_target (3).csv')\n",
        "\n",
        "# Zielvariable hinzufügen\n",
        "bakery_data['demand'] = bakery_target['demand']\n",
        "\n",
        "# Entfernen der 'date' Spalte\n",
        "bakery_data = bakery_data.drop('date', axis=1)\n",
        "\n",
        "# Kategorische Variablen\n",
        "categorical_columns = ['weekday', 'month', 'store', 'product']\n",
        "\n",
        "# Label Encoding der kategorischen Variablen\n",
        "label_encoders = {}\n",
        "for column in categorical_columns:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    bakery_data[column] = label_encoders[column].fit_transform(bakery_data[column])\n",
        "\n",
        "# Alle Features und Zielvariable\n",
        "X = bakery_data.drop(['demand'], axis=1)\n",
        "y = bakery_data['demand']\n",
        "\n",
        "# Normalisierung der Features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testsets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "zYBqRVcFUY9N"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# 1. Daten einlesen und vorbereiten\n",
        "bakery_data = pd.read_csv('/content/bakery_data (3).csv')\n",
        "bakery_target = pd.read_csv('/content/bakery_target (3).csv')\n",
        "\n",
        "# Zielvariable hinzufügen\n",
        "bakery_data['demand'] = bakery_target['demand']\n",
        "\n",
        "# Entfernen der 'date' Spalte (falls vorhanden)\n",
        "if 'date' in bakery_data.columns:\n",
        "    bakery_data = bakery_data.drop('date', axis=1)\n",
        "\n",
        "# Kategorische Variablen\n",
        "categorical_columns = ['weekday', 'month', 'store', 'product']\n",
        "\n",
        "# Label Encoding der kategorischen Variablen\n",
        "label_encoders = {}\n",
        "for column in categorical_columns:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    bakery_data[column] = label_encoders[column].fit_transform(bakery_data[column])\n",
        "\n",
        "# Features (X) und Zielvariable (y)\n",
        "X = bakery_data.drop(['demand'], axis=1)\n",
        "y = bakery_data['demand']\n",
        "\n",
        "# Normalisierung der Features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testsets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Definieren des MLP-Modells ohne Embeddings\n",
        "class MLPWithoutEmbeddings(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(MLPWithoutEmbeddings, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialisiere das MLP-Modell\n",
        "input_size = X_train.shape[1]\n",
        "mlp_model = MLPWithoutEmbeddings(input_size)\n",
        "\n",
        "# Optimizer und Loss-Funktion\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
        "\n",
        "# Konvertiere die Trainingsdaten in Torch-Tensoren\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Trainingsloop\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    mlp_model.train()\n",
        "\n",
        "    # Vorhersagen und Verlust berechnen\n",
        "    optimizer.zero_grad()\n",
        "    outputs = mlp_model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Rückwärtspropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluierung auf dem Testset\n",
        "mlp_model.eval()\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Vorhersagen\n",
        "with torch.no_grad():\n",
        "    y_pred = mlp_model(X_test_tensor).numpy()\n",
        "\n",
        "# Berechnung des Mean Squared Error (MSE) und R²-Wert\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Squared Error (MSE) auf Testset: {mse:.4f}')\n",
        "print(f'R²-Wert auf Testset: {r2:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STvI83pjV2sJ",
        "outputId": "f4442359-8d01-4bcd-c3e8-fef149089bcc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 29254.3301\n",
            "Epoch [20/100], Loss: 29066.5996\n",
            "Epoch [30/100], Loss: 28788.8398\n",
            "Epoch [40/100], Loss: 28382.2305\n",
            "Epoch [50/100], Loss: 27806.1836\n",
            "Epoch [60/100], Loss: 27023.6426\n",
            "Epoch [70/100], Loss: 26009.2188\n",
            "Epoch [80/100], Loss: 24760.3652\n",
            "Epoch [90/100], Loss: 23309.4375\n",
            "Epoch [100/100], Loss: 21732.7812\n",
            "Mean Squared Error (MSE) auf Testset: 21479.6050\n",
            "R²-Wert auf Testset: -0.1163\n"
          ]
        }
      ]
    }
  ]
}