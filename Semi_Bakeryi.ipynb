{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgUtru2qWKOA"
      },
      "source": [
        "# Datenvorbereitung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgXrn2_RWCgH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# 1. Daten einlesen und vorbereiten\n",
        "bakery_data = pd.read_csv('/content/bakery_data (3).csv')\n",
        "bakery_target = pd.read_csv('/content/bakery_target (3).csv')\n",
        "\n",
        "# Zielvariable hinzufügen\n",
        "bakery_data['demand'] = bakery_target['demand']\n",
        "\n",
        "# Entfernen der 'date' Spalte\n",
        "bakery_data = bakery_data.drop('date', axis=1)\n",
        "\n",
        "# Kategorische Variablen\n",
        "categorical_columns = ['weekday', 'month', 'store', 'product']\n",
        "\n",
        "# Label Encoding der kategorischen Variablen\n",
        "label_encoders = {}\n",
        "for column in categorical_columns:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    bakery_data[column] = label_encoders[column].fit_transform(bakery_data[column])\n",
        "\n",
        "# Alle Features und Zielvariable\n",
        "X = bakery_data.drop(['demand'], axis=1)\n",
        "y = bakery_data['demand']\n",
        "\n",
        "# Normalisierung der Features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testsets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loHMgDTlWOqN"
      },
      "source": [
        "# Training eines XGBoost-Modells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaTpt6oeWTgt",
        "outputId": "3e93a3a8-53f4-4ede-b019-2e6dc02ae044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost Mean Squared Error: 6853.27\n",
            "XGBoost R^2 Score: 0.64\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# 2. XGBoost-Modell trainieren\n",
        "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,\n",
        "                          max_depth=5, alpha=10, n_estimators=100, random_state=42)\n",
        "xg_reg.fit(X_train, y_train)\n",
        "\n",
        "# Vorhersagen mit XGBoost-Modell\n",
        "y_pred_xgb = xg_reg.predict(X_test)\n",
        "\n",
        "# Metriken für XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb:.2f}\")\n",
        "print(f\"XGBoost R^2 Score: {r2_xgb:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reKWGQCUWd6A"
      },
      "source": [
        "# Deep Learning-Modell zur Erstellung von Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjEtbvM-WZ3J",
        "outputId": "b6c52e85-0507-46ec-973b-1ab6758ed083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 9255.6152 - val_loss: 3965.5815\n",
            "Epoch 2/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 3902.4695 - val_loss: 3695.6963\n",
            "Epoch 3/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 3688.5000 - val_loss: 3473.3945\n",
            "Epoch 4/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 3526.9077 - val_loss: 3247.4031\n",
            "Epoch 5/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 3289.4629 - val_loss: 3216.5601\n",
            "Epoch 6/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - loss: 3172.7214 - val_loss: 2822.5076\n",
            "Epoch 7/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 2688.6375 - val_loss: 2109.0791\n",
            "Epoch 8/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 1903.6298 - val_loss: 1628.3700\n",
            "Epoch 9/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 1632.0101 - val_loss: 1454.1221\n",
            "Epoch 10/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 1444.9589 - val_loss: 1384.9690\n",
            "Epoch 11/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 1334.8108 - val_loss: 1482.3402\n",
            "Epoch 12/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 1327.4243 - val_loss: 1349.0209\n",
            "Epoch 13/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 1337.9584 - val_loss: 1301.3558\n",
            "Epoch 14/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 1313.0536 - val_loss: 1391.3818\n",
            "Epoch 15/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 1292.3690 - val_loss: 1334.7118\n",
            "Epoch 16/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 1325.8188 - val_loss: 1276.8925\n",
            "Epoch 17/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 1240.1149 - val_loss: 1273.0708\n",
            "Epoch 18/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 1283.2565 - val_loss: 1256.2695\n",
            "Epoch 19/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 1183.7782 - val_loss: 1282.0677\n",
            "Epoch 20/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 1186.0231 - val_loss: 1293.8086\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step\n",
            "\u001b[1m798/798\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense\n",
        "\n",
        "# Embedding Input-Größen\n",
        "embedding_sizes = {'weekday': 7, 'month': 12, 'store': 10, 'product': 50}  # Beispielhafte Input-Dimensionen\n",
        "\n",
        "input_layers = []\n",
        "embedding_layers = []\n",
        "\n",
        "# Für jede kategorische Variable eine Embedding-Schicht definieren\n",
        "for col in categorical_columns:\n",
        "    input_layer = Input(shape=(1,))\n",
        "    input_layers.append(input_layer)\n",
        "    vocab_size = bakery_data[col].nunique() + 1  # Anzahl der eindeutigen Werte + 1\n",
        "    embed_size = embedding_sizes[col]\n",
        "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_size)(input_layer)\n",
        "    embedding_layer = Flatten()(embedding_layer)\n",
        "    embedding_layers.append(embedding_layer)\n",
        "\n",
        "# Numerische Eingaben\n",
        "input_numeric = Input(shape=(X_train.shape[1],))\n",
        "input_layers.append(input_numeric)\n",
        "\n",
        "# Kombination von Embeddings und numerischen Features\n",
        "all_features = Concatenate()(embedding_layers + [input_numeric])\n",
        "\n",
        "# Dichtes neuronales Netz zur Feature-Extraktion\n",
        "x = Dense(64, activation='relu')(all_features)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "output = Dense(1, activation='linear')(x)\n",
        "\n",
        "# Modell zur Extraktion der Embeddings\n",
        "embedding_model = Model(inputs=input_layers, outputs=output)\n",
        "embedding_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Training des Embedding-Modells\n",
        "X_train_inputs = [X_train_cat[col].values for col in categorical_columns] + [X_train]\n",
        "X_test_inputs = [X_test_cat[col].values for col in categorical_columns] + [X_test]\n",
        "embedding_model.fit(X_train_inputs, y_train, epochs=20, batch_size=32, validation_data=(X_test_inputs, y_test))\n",
        "\n",
        "# 3. Embeddings extrahieren\n",
        "feature_extractor = Model(inputs=embedding_model.inputs, outputs=embedding_model.layers[-2].output)\n",
        "X_train_embeddings = feature_extractor.predict(X_train_inputs)\n",
        "X_test_embeddings = feature_extractor.predict(X_test_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6miEShnWnGl"
      },
      "source": [
        "# Training eines XGBoost-Modells mit den Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnm1KMvJWlIW",
        "outputId": "f26ddb33-a2c7-418a-baed-2ffefdc84fba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost (mit Embeddings) Mean Squared Error: 1233.56\n",
            "XGBoost (mit Embeddings) R^2 Score: 0.94\n"
          ]
        }
      ],
      "source": [
        "# 4. XGBoost-Modell trainieren (mit Embeddings als Input)\n",
        "xg_reg_embeddings = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,\n",
        "                                     max_depth=5, alpha=10, n_estimators=100, random_state=42)\n",
        "xg_reg_embeddings.fit(X_train_embeddings, y_train)\n",
        "\n",
        "# Vorhersagen mit XGBoost-Modell (mit Embeddings)\n",
        "y_pred_xgb_embeddings = xg_reg_embeddings.predict(X_test_embeddings)\n",
        "\n",
        "# Metriken für XGBoost (mit Embeddings)\n",
        "mse_xgb_embeddings = mean_squared_error(y_test, y_pred_xgb_embeddings)\n",
        "r2_xgb_embeddings = r2_score(y_test, y_pred_xgb_embeddings)\n",
        "\n",
        "print(f\"XGBoost (mit Embeddings) Mean Squared Error: {mse_xgb_embeddings:.2f}\")\n",
        "print(f\"XGBoost (mit Embeddings) R^2 Score: {r2_xgb_embeddings:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHtqwcEZfYab"
      },
      "source": [
        "# LightGBM ohne Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gwKHfs8fW2u",
        "outputId": "c87bf8ba-0eb5-438f-9282-c9b5d8e0da9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009559 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 558\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 100.194637\n",
            "[CV] END learning_rate=0.01, n_estimators=100, num_leaves=31; total time=   1.2s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007429 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 561\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 99.687671\n",
            "[CV] END learning_rate=0.01, n_estimators=100, num_leaves=31; total time=   0.8s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006025 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 559\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 100.247246\n",
            "[CV] END learning_rate=0.01, n_estimators=100, num_leaves=31; total time=   0.8s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006138 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 558\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 100.194637\n",
            "[CV] END learning_rate=0.01, n_estimators=100, num_leaves=50; total time=   1.0s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006038 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 561\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 99.687671\n",
            "[CV] END learning_rate=0.01, n_estimators=100, num_leaves=50; total time=   1.0s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006148 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 559\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 100.247246\n",
            "[CV] END learning_rate=0.01, n_estimators=100, num_leaves=50; total time=   1.0s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008783 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 558\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 100.194637\n",
            "[CV] END learning_rate=0.01, n_estimators=200, num_leaves=31; total time=   2.3s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009137 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 561\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 99.687671\n",
            "[CV] END learning_rate=0.01, n_estimators=200, num_leaves=31; total time=   2.1s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005965 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 559\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 100.247246\n",
            "[CV] END learning_rate=0.01, n_estimators=200, num_leaves=31; total time=   1.6s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006326 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 558\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 100.194637\n",
            "[CV] END learning_rate=0.01, n_estimators=200, num_leaves=50; total time=   1.9s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006039 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 561\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 99.687671\n",
            "[CV] END learning_rate=0.01, n_estimators=200, num_leaves=50; total time=   1.9s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006017 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 559\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 100.247246\n",
            "[CV] END learning_rate=0.01, n_estimators=200, num_leaves=50; total time=   1.9s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005941 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 558\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 100.194637\n",
            "[CV] END .learning_rate=0.1, n_estimators=100, num_leaves=31; total time=   0.9s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006085 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 561\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 99.687671\n",
            "[CV] END .learning_rate=0.1, n_estimators=100, num_leaves=31; total time=   0.9s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006602 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 559\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 100.247246\n",
            "[CV] END .learning_rate=0.1, n_estimators=100, num_leaves=31; total time=   1.1s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009150 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 558\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 100.194637\n",
            "[CV] END .learning_rate=0.1, n_estimators=100, num_leaves=50; total time=   1.5s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009006 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 561\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 99.687671\n",
            "[CV] END .learning_rate=0.1, n_estimators=100, num_leaves=50; total time=   1.5s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008946 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 559\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 100.247246\n",
            "[CV] END .learning_rate=0.1, n_estimators=100, num_leaves=50; total time=   1.1s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006008 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 558\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 100.194637\n",
            "[CV] END .learning_rate=0.1, n_estimators=200, num_leaves=31; total time=   1.5s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005905 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 561\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 99.687671\n",
            "[CV] END .learning_rate=0.1, n_estimators=200, num_leaves=31; total time=   1.5s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005982 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 559\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 100.247246\n",
            "[CV] END .learning_rate=0.1, n_estimators=200, num_leaves=31; total time=   1.5s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005961 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 558\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 100.194637\n",
            "[CV] END .learning_rate=0.1, n_estimators=200, num_leaves=50; total time=   1.7s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005961 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 561\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 99.687671\n",
            "[CV] END .learning_rate=0.1, n_estimators=200, num_leaves=50; total time=   1.7s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006797 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 559\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 100.247246\n",
            "[CV] END .learning_rate=0.1, n_estimators=200, num_leaves=50; total time=   1.8s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013446 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 567\n",
            "[LightGBM] [Info] Number of data points in the train set: 102060, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score 100.043185\n",
            "Beste Parameter für LightGBM: {'learning_rate': 0.1, 'n_estimators': 200, 'num_leaves': 50}\n",
            "LightGBM Mean Squared Error: 1329.10\n",
            "LightGBM R^2 Score: 0.93\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# LightGBM-Modell ohne Embeddings\n",
        "lgb_reg = lgb.LGBMRegressor()\n",
        "\n",
        "# Hyperparameter-Tuning für LightGBM\n",
        "param_grid_lgb = {\n",
        "    'num_leaves': [31, 50],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'n_estimators': [100, 200]\n",
        "}\n",
        "grid_search_lgb = GridSearchCV(estimator=lgb_reg, param_grid=param_grid_lgb, cv=3, scoring='neg_mean_squared_error', verbose=2)\n",
        "grid_search_lgb.fit(X_train, y_train)\n",
        "\n",
        "# Beste Parameter\n",
        "print(\"Beste Parameter für LightGBM:\", grid_search_lgb.best_params_)\n",
        "\n",
        "# Modell mit besten Parametern\n",
        "lgb_reg_best = grid_search_lgb.best_estimator_\n",
        "\n",
        "# Vorhersagen mit LightGBM-Modell\n",
        "y_pred_lgb = lgb_reg_best.predict(X_test)\n",
        "\n",
        "# Metriken für LightGBM\n",
        "mse_lgb = mean_squared_error(y_test, y_pred_lgb)\n",
        "r2_lgb = r2_score(y_test, y_pred_lgb)\n",
        "\n",
        "print(f\"LightGBM Mean Squared Error: {mse_lgb:.2f}\")\n",
        "print(f\"LightGBM R^2 Score: {r2_lgb:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQDCAczzf0OQ"
      },
      "source": [
        "# LightGBM mit Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK8X1Nh7fyKa",
        "outputId": "4a1ff132-81dd-4ea9-817e-7724167cab3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017055 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 100.194637\n",
            "[CV] END learning_rate=0.01, n_estimators=100, num_leaves=31; total time=   1.8s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016420 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 99.687671\n",
            "[CV] END learning_rate=0.01, n_estimators=100, num_leaves=31; total time=   1.8s\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125079 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 100.247246\n",
            "[CV] END learning_rate=0.01, n_estimators=100, num_leaves=31; total time=   3.5s\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050055 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 100.194637\n",
            "[CV] END learning_rate=0.01, n_estimators=100, num_leaves=50; total time=   2.9s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016583 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 99.687671\n",
            "[CV] END learning_rate=0.01, n_estimators=100, num_leaves=50; total time=   2.1s\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030279 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 100.247246\n",
            "[CV] END learning_rate=0.01, n_estimators=100, num_leaves=50; total time=   2.2s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016780 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 100.194637\n",
            "[CV] END learning_rate=0.01, n_estimators=200, num_leaves=31; total time=   6.2s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025864 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 99.687671\n",
            "[CV] END learning_rate=0.01, n_estimators=200, num_leaves=31; total time=   4.2s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016316 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 100.247246\n",
            "[CV] END learning_rate=0.01, n_estimators=200, num_leaves=31; total time=   3.3s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017813 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 100.194637\n",
            "[CV] END learning_rate=0.01, n_estimators=200, num_leaves=50; total time=   3.9s\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031905 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 99.687671\n",
            "[CV] END learning_rate=0.01, n_estimators=200, num_leaves=50; total time=   5.4s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016892 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 100.247246\n",
            "[CV] END learning_rate=0.01, n_estimators=200, num_leaves=50; total time=   3.9s\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029507 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 100.194637\n",
            "[CV] END .learning_rate=0.1, n_estimators=100, num_leaves=31; total time=   1.7s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016640 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 99.687671\n",
            "[CV] END .learning_rate=0.1, n_estimators=100, num_leaves=31; total time=   1.6s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017946 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 100.247246\n",
            "[CV] END .learning_rate=0.1, n_estimators=100, num_leaves=31; total time=   1.6s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018786 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 100.194637\n",
            "[CV] END .learning_rate=0.1, n_estimators=100, num_leaves=50; total time=   2.2s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027084 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 99.687671\n",
            "[CV] END .learning_rate=0.1, n_estimators=100, num_leaves=50; total time=   2.8s\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031214 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 100.247246\n",
            "[CV] END .learning_rate=0.1, n_estimators=100, num_leaves=50; total time=   2.1s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018902 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 100.194637\n",
            "[CV] END .learning_rate=0.1, n_estimators=200, num_leaves=31; total time=   2.8s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018115 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 99.687671\n",
            "[CV] END .learning_rate=0.1, n_estimators=200, num_leaves=31; total time=   2.8s\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032305 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 100.247246\n",
            "[CV] END .learning_rate=0.1, n_estimators=200, num_leaves=31; total time=   3.3s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025744 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 100.194637\n",
            "[CV] END .learning_rate=0.1, n_estimators=200, num_leaves=50; total time=   4.5s\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031831 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 99.687671\n",
            "[CV] END .learning_rate=0.1, n_estimators=200, num_leaves=50; total time=   3.4s\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017078 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 68040, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 100.247246\n",
            "[CV] END .learning_rate=0.1, n_estimators=200, num_leaves=50; total time=   3.4s\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045646 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7650\n",
            "[LightGBM] [Info] Number of data points in the train set: 102060, number of used features: 30\n",
            "[LightGBM] [Info] Start training from score 100.043185\n",
            "Beste Parameter für LightGBM (mit Embeddings): {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 50}\n",
            "LightGBM (mit Embeddings) Mean Squared Error: 1197.59\n",
            "LightGBM (mit Embeddings) R^2 Score: 0.94\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# LightGBM-Modell mit Embeddings als Input\n",
        "lgb_reg_embeddings = lgb.LGBMRegressor()\n",
        "\n",
        "# Hyperparameter-Tuning für LightGBM mit Embeddings\n",
        "grid_search_lgb_embeddings = GridSearchCV(estimator=lgb_reg_embeddings, param_grid=param_grid_lgb, cv=3, scoring='neg_mean_squared_error', verbose=2)\n",
        "grid_search_lgb_embeddings.fit(X_train_embeddings, y_train)\n",
        "\n",
        "# Beste Parameter\n",
        "print(\"Beste Parameter für LightGBM (mit Embeddings):\", grid_search_lgb_embeddings.best_params_)\n",
        "\n",
        "# Modell mit besten Parametern\n",
        "lgb_reg_best_embeddings = grid_search_lgb_embeddings.best_estimator_\n",
        "\n",
        "# Vorhersagen mit LightGBM-Modell (mit Embeddings)\n",
        "y_pred_lgb_embeddings = lgb_reg_best_embeddings.predict(X_test_embeddings)\n",
        "\n",
        "# Metriken für LightGBM (mit Embeddings)\n",
        "mse_lgb_embeddings = mean_squared_error(y_test, y_pred_lgb_embeddings)\n",
        "r2_lgb_embeddings = r2_score(y_test, y_pred_lgb_embeddings)\n",
        "\n",
        "print(f\"LightGBM (mit Embeddings) Mean Squared Error: {mse_lgb_embeddings:.2f}\")\n",
        "print(f\"LightGBM (mit Embeddings) R^2 Score: {r2_lgb_embeddings:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asn7iUJVhHuq"
      },
      "source": [
        "# XGBoost besser ohne Embeddings? Hyperparameter-Tuning für ein XGBoost-Modell, Grid Search Cross-Validation (CV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM_8PlQ5hIR2",
        "outputId": "d43534c4-5d01-4d3b-b42b-fc3b1fce142c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=3, n_estimators=200; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=3, n_estimators=200; total time=   1.9s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=5, n_estimators=100; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=5, n_estimators=200; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=5, n_estimators=200; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=5, n_estimators=200; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=7, n_estimators=100; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=7, n_estimators=100; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=7, n_estimators=100; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=7, n_estimators=200; total time=   3.8s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=7, n_estimators=200; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=7, n_estimators=200; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=3, n_estimators=100; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=3, n_estimators=200; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=5, n_estimators=200; total time=   2.7s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=5, n_estimators=200; total time=   1.9s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=5, n_estimators=200; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=7, n_estimators=200; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=7, n_estimators=200; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.1, max_depth=7, n_estimators=200; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=3, n_estimators=200; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=3, n_estimators=200; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=3, n_estimators=200; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=5, n_estimators=100; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=5, n_estimators=100; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=5, n_estimators=100; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=5, n_estimators=200; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=5, n_estimators=200; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=5, n_estimators=200; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=7, n_estimators=100; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=7, n_estimators=100; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=7, n_estimators=100; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=7, n_estimators=200; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=7, n_estimators=200; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=7, n_estimators=200; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, n_estimators=100; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, n_estimators=100; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, n_estimators=200; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, n_estimators=200; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=5, n_estimators=200; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, n_estimators=100; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, n_estimators=100; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, n_estimators=100; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, n_estimators=200; total time=   4.3s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, n_estimators=200; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, n_estimators=200; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, n_estimators=200; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, n_estimators=200; total time=   3.8s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, n_estimators=200; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, n_estimators=200; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, n_estimators=200; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, n_estimators=200; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=3, n_estimators=200; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=3, n_estimators=200; total time=   3.4s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=3, n_estimators=200; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=5, n_estimators=100; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=5, n_estimators=100; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=5, n_estimators=100; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=5, n_estimators=200; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=5, n_estimators=200; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=5, n_estimators=200; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=100; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=100; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=100; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=200; total time=   2.9s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=200; total time=   3.1s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=200; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=100; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200; total time=   4.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=200; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200; total time=   1.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200; total time=   2.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=100; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=100; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=200; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, n_estimators=200; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, n_estimators=200; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, n_estimators=200; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, n_estimators=100; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, n_estimators=100; total time=   2.9s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, n_estimators=200; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, n_estimators=200; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, n_estimators=200; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=3, n_estimators=200; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=3, n_estimators=200; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=3, n_estimators=200; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=5, n_estimators=100; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=5, n_estimators=100; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=5, n_estimators=100; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=5, n_estimators=200; total time=   2.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=5, n_estimators=200; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=5, n_estimators=200; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=7, n_estimators=100; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=7, n_estimators=100; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=7, n_estimators=100; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=7, n_estimators=200; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=7, n_estimators=200; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=7, n_estimators=200; total time=   1.4s\n",
            "XGBoost Mean Squared Error: 1353.44\n",
            "XGBoost R^2 Score: 0.93\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Daten einlesen und vorbereiten\n",
        "bakery_data = pd.read_csv('/content/bakery_data (3).csv')\n",
        "bakery_target = pd.read_csv('/content/bakery_target (3).csv')\n",
        "bakery_data['demand'] = bakery_target['demand']\n",
        "bakery_data = bakery_data.drop('date', axis=1)\n",
        "\n",
        "# Kategorische Variablen\n",
        "categorical_columns = ['weekday', 'month', 'store', 'product']\n",
        "label_encoders = {}\n",
        "for column in categorical_columns:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    bakery_data[column] = label_encoders[column].fit_transform(bakery_data[column])\n",
        "\n",
        "# Alle Features und Zielvariable\n",
        "X = bakery_data.drop(['demand'], axis=1)\n",
        "y = bakery_data['demand']\n",
        "\n",
        "# Normalisierung der Features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testsets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# XGBoost-Modell\n",
        "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Hyperparameter-Tuning\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'colsample_bytree': [0.3, 0.5, 0.7]\n",
        "}\n",
        "grid_search_xgb = GridSearchCV(estimator=xg_reg, param_grid=param_grid_xgb, cv=3, scoring='neg_mean_squared_error', verbose=2)\n",
        "grid_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Bestes Modell und Vorhersagen\n",
        "best_xgb = grid_search_xgb.best_estimator_\n",
        "y_pred_xgb = best_xgb.predict(X_test)\n",
        "\n",
        "# Metriken für XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb:.2f}\")\n",
        "print(f\"XGBoost R^2 Score: {r2_xgb:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHDpgDG9hmPO"
      },
      "source": [
        "# XGBoost mit Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEjcOpZOhjZl",
        "outputId": "9144da81-49f9-4a91-b128-b77924510a43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 10080.4268 - val_loss: 4012.2776\n",
            "Epoch 2/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 3918.5989 - val_loss: 3654.6050\n",
            "Epoch 3/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 3616.8767 - val_loss: 3496.3254\n",
            "Epoch 4/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 4ms/step - loss: 3434.4839 - val_loss: 3295.3628\n",
            "Epoch 5/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 3327.0933 - val_loss: 3151.4429\n",
            "Epoch 6/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 3223.4062 - val_loss: 3030.6929\n",
            "Epoch 7/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 3118.7805 - val_loss: 2913.2332\n",
            "Epoch 8/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 2748.7500 - val_loss: 2611.1807\n",
            "Epoch 9/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 2494.2478 - val_loss: 2035.4816\n",
            "Epoch 10/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 1975.4808 - val_loss: 1704.4662\n",
            "Epoch 11/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 1613.6484 - val_loss: 1552.9377\n",
            "Epoch 12/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 1511.3234 - val_loss: 1472.1041\n",
            "Epoch 13/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 1423.9099 - val_loss: 1429.4635\n",
            "Epoch 14/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 1405.1206 - val_loss: 1397.0900\n",
            "Epoch 15/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 1326.8370 - val_loss: 1373.3011\n",
            "Epoch 16/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 1312.2828 - val_loss: 1332.2775\n",
            "Epoch 17/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 1333.5028 - val_loss: 1350.6283\n",
            "Epoch 18/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 1340.8171 - val_loss: 1352.8549\n",
            "Epoch 19/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 1253.3591 - val_loss: 1309.8625\n",
            "Epoch 20/20\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 1250.3809 - val_loss: 1266.0591\n",
            "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step\n",
            "\u001b[1m798/798\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
            "XGBoost (mit Embeddings) Mean Squared Error: 1330.67\n",
            "XGBoost (mit Embeddings) R^2 Score: 0.93\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense\n",
        "\n",
        "# Embedding Input-Größen\n",
        "embedding_sizes = {'weekday': 7, 'month': 12, 'store': 10, 'product': 50}\n",
        "\n",
        "input_layers = []\n",
        "embedding_layers = []\n",
        "\n",
        "# Für jede kategorische Variable eine Embedding-Schicht definieren\n",
        "for col in categorical_columns:\n",
        "    input_layer = Input(shape=(1,))\n",
        "    input_layers.append(input_layer)\n",
        "    vocab_size = bakery_data[col].nunique() + 1\n",
        "    embed_size = embedding_sizes[col]\n",
        "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_size)(input_layer)\n",
        "    embedding_layer = Flatten()(embedding_layer)\n",
        "    embedding_layers.append(embedding_layer)\n",
        "\n",
        "# Numerische Eingaben\n",
        "input_numeric = Input(shape=(X_train.shape[1] - len(categorical_columns),))\n",
        "input_layers.append(input_numeric)\n",
        "\n",
        "# Kombination von Embeddings und numerischen Features\n",
        "all_features = Concatenate()(embedding_layers + [input_numeric])\n",
        "\n",
        "# Dichtes neuronales Netz zur Feature-Extraktion\n",
        "x = Dense(64, activation='relu')(all_features)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "output = Dense(1, activation='linear')(x)\n",
        "\n",
        "# Modell zur Extraktion der Embeddings\n",
        "embedding_model = Model(inputs=input_layers, outputs=output)\n",
        "embedding_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Daten vorbereiten für das Training des Embedding-Modells\n",
        "X_train_cat = [X_train[col].values for col in categorical_columns]\n",
        "X_test_cat = [X_test[col].values for col in categorical_columns]\n",
        "X_train_num = X_train.drop(columns=categorical_columns)\n",
        "X_test_num = X_test.drop(columns=categorical_columns)\n",
        "\n",
        "X_train_inputs = X_train_cat + [X_train_num.values]\n",
        "X_test_inputs = X_test_cat + [X_test_num.values]\n",
        "\n",
        "# Training des Embedding-Modells\n",
        "embedding_model.fit(X_train_inputs, y_train, epochs=20, batch_size=32, validation_data=(X_test_inputs, y_test))\n",
        "\n",
        "# 4. Embeddings extrahieren\n",
        "feature_extractor = Model(inputs=embedding_model.inputs, outputs=embedding_model.layers[-2].output)\n",
        "X_train_embeddings = feature_extractor.predict(X_train_inputs)\n",
        "X_test_embeddings = feature_extractor.predict(X_test_inputs)\n",
        "\n",
        "# 5. XGBoost-Modell trainieren (mit Embeddings als Input)\n",
        "xg_reg_embeddings = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,\n",
        "                                     max_depth=5, alpha=10, n_estimators=100, random_state=42)\n",
        "xg_reg_embeddings.fit(X_train_embeddings, y_train)\n",
        "\n",
        "# Vorhersagen mit XGBoost-Modell (mit Embeddings)\n",
        "y_pred_xgb_embeddings = xg_reg_embeddings.predict(X_test_embeddings)\n",
        "\n",
        "# Metriken für XGBoost (mit Embeddings)\n",
        "mse_xgb_embeddings = mean_squared_error(y_test, y_pred_xgb_embeddings)\n",
        "r2_xgb_embeddings = r2_score(y_test, y_pred_xgb_embeddings)\n",
        "\n",
        "print(f\"XGBoost (mit Embeddings) Mean Squared Error: {mse_xgb_embeddings:.2f}\")\n",
        "print(f\"XGBoost (mit Embeddings) R^2 Score: {r2_xgb_embeddings:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POAf0EG0m9Al"
      },
      "source": [
        "# Ergebnisse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2C6TBaZm9mR",
        "outputId": "1339f6df-6e47-4b6b-fd21-f1c63ecc4d96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                       Model  Mean Squared Error  R^2 Score\n",
            "0                    XGBoost             6853.27       0.64\n",
            "1   XGBoost (mit Embeddings)             1233.56       0.94\n",
            "2                   LightGBM             1329.10       0.93\n",
            "3  LightGBM (mit Embeddings)             1197.59       0.94\n",
            "4                    XGBoost             1353.44       0.93\n",
            "5   XGBoost (mit Embeddings)             1330.67       0.93\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ergebnisse in einem Dictionary speichern\n",
        "results = {\n",
        "    \"Model\": [\"XGBoost\", \"XGBoost (mit Embeddings)\", \"LightGBM\", \"LightGBM (mit Embeddings)\", \"XGBoost\", \"XGBoost (mit Embeddings)\"],\n",
        "    \"Mean Squared Error\": [6853.27, 1233.56, 1329.10, 1197.59, 1353.44, 1330.67],\n",
        "    \"R^2 Score\": [0.64, 0.94, 0.93, 0.94, 0.93, 0.93]\n",
        "}\n",
        "\n",
        "# Erstellen eines DataFrames\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Tabelle anzeigen\n",
        "print(df_results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}